{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f005da39-586c-44b5-9594-3a6bd809d82f",
   "metadata": {},
   "source": [
    "Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da6313c3-230e-436e-8e66-4c952c31320c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "def logging_process(log_file=\"script/log/info.log\"):\n",
    "    # Configure logging\n",
    "    os.makedirs(os.path.dirname(log_file), exist_ok=True)\n",
    "    logging.basicConfig(\n",
    "        filename=log_file,\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    "    )\n",
    "    logger = logging.getLogger()\n",
    "    return logger\n",
    "\n",
    "def load_log_msg(spark: SparkSession, log_msg):\n",
    "\n",
    "    DB_URL = \"jdbc:postgresql://pipeline_db:5432/etl_log\"\n",
    "    table_name = \"etl_log\"\n",
    "\n",
    "    # set config\n",
    "    connection_properties = {\n",
    "        \"user\":\"postgres\",\n",
    "        \"password\":\"cobapassword\",\n",
    "        \"driver\": \"org.postgresql.Driver\"\n",
    "    }\n",
    "\n",
    "    log_msg.write.jdbc(url = DB_URL,\n",
    "                  table = table_name,\n",
    "                  mode = \"append\",\n",
    "                  properties = connection_properties)\n",
    "\n",
    "\n",
    "def init_spark_session():\n",
    "    spark = SparkSession.builder.appName(\n",
    "        \"Exercise Data Pipeline Week_6\"\n",
    "    ).getOrCreate()\n",
    "\n",
    "    # handle legacy time parser\n",
    "    spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801feb61-03c4-4e6e-96ea-f940e99ed802",
   "metadata": {},
   "source": [
    "staging_extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79cc84a2-62c4-45e8-8ca7-2cd5b8f4e374",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.helper import logging_process, init_spark_session, load_log_msg\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import pyspark\n",
    "\n",
    "logging_process()\n",
    "\n",
    "\n",
    "def extract_data(data_name: str, format_data: str) -> pyspark.sql.DataFrame:\n",
    "    spark = init_spark_session()\n",
    "\n",
    "    DB_URL = \"jdbc:postgresql://source_db:5432/startup_investments\"\n",
    "    DB_USER = \"postgres\"\n",
    "    DB_PASS = \"cobapassword\"\n",
    "\n",
    "    jdbc_url = DB_URL\n",
    "    connection_properties = {\n",
    "        \"user\": DB_USER,\n",
    "        \"password\": DB_PASS,\n",
    "        \"driver\": \"org.postgresql.Driver\"\n",
    "    }\n",
    "\n",
    "    current_timestamp = datetime.now()\n",
    "    log_message = None\n",
    "\n",
    "    try:\n",
    "        logging.info(f\"===== Start Extracting {data_name} data =====\")\n",
    "\n",
    "        if format_data.lower() == \"csv\":\n",
    "            df = spark.read.csv(f\"data/{data_name}.csv\", header=True)\n",
    "\n",
    "        elif format_data.lower() == \"db\":\n",
    "            df = spark.read.jdbc(\n",
    "                url=jdbc_url,\n",
    "                table=data_name,\n",
    "                properties=connection_properties\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Format data not supported yet\")\n",
    "\n",
    "        logging.info(f\"===== Finish Extracting {data_name} data =====\")\n",
    "\n",
    "        # Log success\n",
    "        log_message = spark.sparkContext.parallelize([(\n",
    "            \"sources\", \"extract\", \"success\", format_data, data_name, current_timestamp\n",
    "        )]).toDF([\"step\", \"process\", \"status\", \"source\", \"table_name\", \"etl_date\"])\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(\"====== Failed to Extract Data ======\")\n",
    "        logging.error(str(e))\n",
    "\n",
    "        # Log failure\n",
    "        log_message = spark.sparkContext.parallelize([(\n",
    "            \"sources\", \"extraction\", \"failed\", format_data, data_name, current_timestamp, str(e)\n",
    "        )]).toDF([\"step\", \"process\", \"status\", \"source\", \"table_name\", \"etl_date\", \"error_msg\"])\n",
    "\n",
    "        raise\n",
    "\n",
    "    finally:\n",
    "        if log_message:\n",
    "            try:\n",
    "                load_log_msg(spark, log_message)\n",
    "            except Exception as log_err:\n",
    "                logging.error(f\"Failed to write log to DB: {log_err}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bff9063-160e-428e-bada-3a61b600311a",
   "metadata": {},
   "source": [
    "staging_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c03e4fdb-7098-4900-9eb6-a75baf398c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.helper import logging_process, init_spark_session, load_log_msg\n",
    "import logging\n",
    "import pyspark\n",
    "from sqlalchemy import create_engine, text\n",
    "from datetime import datetime\n",
    "\n",
    "logging_process()\n",
    "\n",
    "\n",
    "def load_data(df_result: pyspark.sql.DataFrame, table_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Function to truncate a table using SQLAlchemy and then load data into it using PySpark,\n",
    "    while logging success/failure ETL events into a log table.\n",
    "    \"\"\"\n",
    "\n",
    "    logging_process()\n",
    "\n",
    "    spark = init_spark_session()\n",
    "    current_timestamp = datetime.now()\n",
    "    log_message = None\n",
    "\n",
    "    # DB config\n",
    "    DB_HOST = \"pipeline_db\"\n",
    "    DB_PORT = \"5432\"\n",
    "    DB_NAME = \"staging\"\n",
    "    DB_USER = \"postgres\"\n",
    "    DB_PASS = \"cobapassword\"\n",
    "\n",
    "    jdbc_url = f\"jdbc:postgresql://{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "    sqlalchemy_url = f\"postgresql://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "\n",
    "    connection_properties = {\n",
    "        \"user\": DB_USER,\n",
    "        \"password\": DB_PASS,\n",
    "        \"driver\": \"org.postgresql.Driver\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        logging.info(\"===== Start Load data to the database =====\")\n",
    "\n",
    "        # TRUNCATE TABLE using SQLAlchemy\n",
    "        engine = create_engine(sqlalchemy_url)\n",
    "        with engine.connect() as connection:\n",
    "            connection.execute(text(f\"TRUNCATE TABLE {table_name} CASCADE\"))\n",
    "            logging.info(f\"===== Truncated table {table_name} successfully =====\")\n",
    "\n",
    "        # Load data using PySpark\n",
    "        df_result.write.jdbc(\n",
    "            url=jdbc_url,\n",
    "            table=table_name,\n",
    "            mode=\"append\",\n",
    "            properties=connection_properties,\n",
    "        )\n",
    "\n",
    "        logging.info(\"===== Finish Load data to the database =====\")\n",
    "\n",
    "        # SUCCESS log\n",
    "        log_message = spark.sparkContext.parallelize([(\n",
    "            \"targets\", \"load\", \"success\", \"db\", table_name, current_timestamp\n",
    "        )]).toDF([\"step\", \"process\", \"status\", \"source\", \"table_name\", \"etl_date\"])\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(\"===== Failed Load data to the database =====\")\n",
    "        logging.error(str(e))\n",
    "\n",
    "        # FAILURE log\n",
    "        log_message = spark.sparkContext.parallelize([(\n",
    "            \"targets\", \"load\", \"failed\", \"db\", table_name, current_timestamp, str(e)\n",
    "        )]).toDF([\"step\", \"process\", \"status\", \"source\", \"table_name\", \"etl_date\", \"error_msg\"])\n",
    "\n",
    "        raise\n",
    "\n",
    "    finally:\n",
    "        if log_message is not None:\n",
    "            try:\n",
    "                load_log_msg(spark, log_message)\n",
    "                logging.info(\"ETL log inserted successfully\")\n",
    "            except Exception as log_err:\n",
    "                logging.error(f\"Failed to write log to DB: {log_err}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38b4457-16b3-4798-9ffa-49955cf10f55",
   "metadata": {},
   "source": [
    "staging_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ab20ee0-5a8b-4fd2-98d4-b265af5fec62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Initialize logging\n",
    "logging_process()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.info(\"===== Start Banking Data Pipeline =====\")\n",
    "\n",
    "    try:\n",
    "        # Extract data from CSV and database\n",
    "        df_people = extract_data(data_name=\"people\", format_data=\"csv\")\n",
    "        df_relationships = extract_data(data_name=\"relationships\", format_data=\"csv\")\n",
    "        df_acquisition = extract_data(data_name=\"acquisition\", format_data=\"db\")\n",
    "        df_funds = extract_data(data_name=\"funds\", format_data=\"db\")\n",
    "        df_funding_rounds = extract_data(data_name=\"funding_rounds\", format_data=\"db\")\n",
    "        df_company = extract_data(data_name=\"company\", format_data=\"db\")\n",
    "        df_investments = extract_data(data_name=\"investments\", format_data=\"db\")\n",
    "        df_ipos = extract_data(data_name=\"ipos\", format_data=\"db\")\n",
    "\n",
    "        # Transform each dataset separately\n",
    "        # df_transactions = transform_data(df_transactions, \"transactions\")\n",
    "\n",
    "        # Load each transformed dataset into the data warehouse\n",
    "        load_data(df_people, table_name=\"people\")\n",
    "        load_data(df_relationships, table_name=\"relationships\")\n",
    "        load_data(df_acquisition, table_name=\"acquisition\")\n",
    "        load_data(df_funds, table_name=\"funds\")\n",
    "        load_data(df_funding_rounds, table_name=\"funding_rounds\")\n",
    "        load_data(df_company, table_name=\"company\")\n",
    "        load_data(df_investments, table_name=\"investments\")\n",
    "        load_data(df_ipos, table_name=\"ipos\")\n",
    "\n",
    "        logging.info(\"===== Finish Investment Data Pipeline =====\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(\"===== Data Pipeline Failed =====\")\n",
    "        logging.error(e)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b3f6d96-c474-48e1-8547-2f9b7ae2feae",
   "metadata": {},
   "outputs": [],
   "source": [
    "        load_data(df_people, table_name=\"people\")\n",
    "        load_data(df_relationships, table_name=\"relationships\")\n",
    "        load_data(df_acquisition, table_name=\"acquisition\")\n",
    "        load_data(df_funds, table_name=\"funds\")\n",
    "        load_data(df_funding_rounds, table_name=\"funding_rounds\")\n",
    "        load_data(df_company, table_name=\"company\")\n",
    "        load_data(df_investments, table_name=\"investments\")\n",
    "        load_data(df_ipos, table_name=\"ipos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60516a8e-f4eb-4337-b071-ce9d217ce6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ipos = extract_data(data_name=\"ipos\", format_data=\"db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1eb70418-cd57-4c65-ab2c-2b0ea3566243",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data(df_ipos, table_name=\"ipos\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
