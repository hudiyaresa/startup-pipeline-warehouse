{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f005da39-586c-44b5-9594-3a6bd809d82f",
   "metadata": {},
   "source": [
    "Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da6313c3-230e-436e-8e66-4c952c31320c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "def logging_process(log_file=\"script/log/info.log\"):\n",
    "    # Configure logging\n",
    "    os.makedirs(os.path.dirname(log_file), exist_ok=True)\n",
    "    logging.basicConfig(\n",
    "        filename=log_file,\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    "    )\n",
    "    logger = logging.getLogger()\n",
    "    return logger\n",
    "\n",
    "def load_log_msg(spark: SparkSession, log_msg):\n",
    "\n",
    "    DB_URL = \"jdbc:postgresql://pipeline_db:5432/etl_log\"\n",
    "    table_name = \"etl_log\"\n",
    "\n",
    "    # set config\n",
    "    connection_properties = {\n",
    "        \"user\":\"postgres\",\n",
    "        \"password\":\"cobapassword\",\n",
    "        \"driver\": \"org.postgresql.Driver\"\n",
    "    }\n",
    "\n",
    "    log_msg.write.jdbc(url = DB_URL,\n",
    "                  table = table_name,\n",
    "                  mode = \"append\",\n",
    "                  properties = connection_properties)\n",
    "\n",
    "\n",
    "def init_spark_session():\n",
    "    spark = SparkSession.builder.appName(\n",
    "        \"Exercise Data Pipeline Week_6\"\n",
    "    ).getOrCreate()\n",
    "\n",
    "    # handle legacy time parser\n",
    "    spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801feb61-03c4-4e6e-96ea-f940e99ed802",
   "metadata": {},
   "source": [
    "staging_extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79cc84a2-62c4-45e8-8ca7-2cd5b8f4e374",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.helper import logging_process, init_spark_session, load_log_msg\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import pyspark\n",
    "\n",
    "logging_process()\n",
    "\n",
    "\n",
    "def extract_data(data_name: str, format_data: str) -> pyspark.sql.DataFrame:\n",
    "    spark = init_spark_session()\n",
    "\n",
    "    DB_URL = \"jdbc:postgresql://source_db:5432/startup_investments\"\n",
    "    DB_USER = \"postgres\"\n",
    "    DB_PASS = \"cobapassword\"\n",
    "\n",
    "    jdbc_url = DB_URL\n",
    "    connection_properties = {\n",
    "        \"user\": DB_USER,\n",
    "        \"password\": DB_PASS,\n",
    "        \"driver\": \"org.postgresql.Driver\"\n",
    "    }\n",
    "\n",
    "    current_timestamp = datetime.now()\n",
    "    log_message = None\n",
    "\n",
    "    try:\n",
    "        logging.info(f\"===== Start Extracting {data_name} data =====\")\n",
    "\n",
    "        if format_data.lower() == \"csv\":\n",
    "            df = spark.read.csv(f\"data/{data_name}.csv\", header=True)\n",
    "\n",
    "        elif format_data.lower() == \"db\":\n",
    "            df = spark.read.jdbc(\n",
    "                url=jdbc_url,\n",
    "                table=data_name,\n",
    "                properties=connection_properties\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Format data not supported yet\")\n",
    "\n",
    "        logging.info(f\"===== Finish Extracting {data_name} data =====\")\n",
    "\n",
    "        # Log success\n",
    "        log_message = spark.sparkContext.parallelize([(\n",
    "            \"sources\", \"extract\", \"success\", format_data, data_name, current_timestamp\n",
    "        )]).toDF([\"step\", \"process\", \"status\", \"source\", \"table_name\", \"etl_date\"])\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(\"====== Failed to Extract Data ======\")\n",
    "        logging.error(str(e))\n",
    "\n",
    "        # Log failure\n",
    "        log_message = spark.sparkContext.parallelize([(\n",
    "            \"sources\", \"extraction\", \"failed\", format_data, data_name, current_timestamp, str(e)\n",
    "        )]).toDF([\"step\", \"process\", \"status\", \"source\", \"table_name\", \"etl_date\", \"error_msg\"])\n",
    "\n",
    "        raise\n",
    "\n",
    "    finally:\n",
    "        if log_message:\n",
    "            try:\n",
    "                load_log_msg(spark, log_message)\n",
    "            except Exception as log_err:\n",
    "                logging.error(f\"Failed to write log to DB: {log_err}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bff9063-160e-428e-bada-3a61b600311a",
   "metadata": {},
   "source": [
    "staging_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c03e4fdb-7098-4900-9eb6-a75baf398c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.helper import logging_process, init_spark_session, load_log_msg\n",
    "import logging\n",
    "import pyspark\n",
    "from sqlalchemy import create_engine, text\n",
    "from datetime import datetime\n",
    "\n",
    "logging_process()\n",
    "\n",
    "\n",
    "def load_data(df_result: pyspark.sql.DataFrame, table_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Function to truncate a table using SQLAlchemy and then load data into it using PySpark,\n",
    "    while logging success/failure ETL events into a log table.\n",
    "    \"\"\"\n",
    "\n",
    "    logging_process()\n",
    "\n",
    "    spark = init_spark_session()\n",
    "    current_timestamp = datetime.now()\n",
    "    log_message = None\n",
    "\n",
    "    # DB config\n",
    "    DB_HOST = \"pipeline_db\"\n",
    "    DB_PORT = \"5432\"\n",
    "    DB_NAME = \"staging\"\n",
    "    DB_USER = \"postgres\"\n",
    "    DB_PASS = \"cobapassword\"\n",
    "\n",
    "    jdbc_url = f\"jdbc:postgresql://{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "    sqlalchemy_url = f\"postgresql://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "\n",
    "    connection_properties = {\n",
    "        \"user\": DB_USER,\n",
    "        \"password\": DB_PASS,\n",
    "        \"driver\": \"org.postgresql.Driver\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        logging.info(\"===== Start Load data to the database =====\")\n",
    "\n",
    "        # TRUNCATE TABLE using SQLAlchemy\n",
    "        engine = create_engine(sqlalchemy_url)\n",
    "        with engine.connect() as connection:\n",
    "            connection.execute(text(f\"TRUNCATE TABLE {table_name} CASCADE\"))\n",
    "            logging.info(f\"===== Truncated table {table_name} successfully =====\")\n",
    "\n",
    "        # Load data using PySpark\n",
    "        df_result.write.jdbc(\n",
    "            url=jdbc_url,\n",
    "            table=table_name,\n",
    "            mode=\"append\",\n",
    "            properties=connection_properties,\n",
    "        )\n",
    "\n",
    "        logging.info(\"===== Finish Load data to the database =====\")\n",
    "\n",
    "        # SUCCESS log\n",
    "        log_message = spark.sparkContext.parallelize([(\n",
    "            \"targets\", \"load\", \"success\", \"db\", table_name, current_timestamp\n",
    "        )]).toDF([\"step\", \"process\", \"status\", \"source\", \"table_name\", \"etl_date\"])\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(\"===== Failed Load data to the database =====\")\n",
    "        logging.error(str(e))\n",
    "\n",
    "        # FAILURE log\n",
    "        log_message = spark.sparkContext.parallelize([(\n",
    "            \"targets\", \"load\", \"failed\", \"db\", table_name, current_timestamp, str(e)\n",
    "        )]).toDF([\"step\", \"process\", \"status\", \"source\", \"table_name\", \"etl_date\", \"error_msg\"])\n",
    "\n",
    "        raise\n",
    "\n",
    "    finally:\n",
    "        if log_message is not None:\n",
    "            try:\n",
    "                load_log_msg(spark, log_message)\n",
    "                logging.info(\"ETL log inserted successfully\")\n",
    "            except Exception as log_err:\n",
    "                logging.error(f\"Failed to write log to DB: {log_err}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38b4457-16b3-4798-9ffa-49955cf10f55",
   "metadata": {},
   "source": [
    "staging_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ab20ee0-5a8b-4fd2-98d4-b265af5fec62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Initialize logging\n",
    "logging_process()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.info(\"===== Start Banking Data Pipeline =====\")\n",
    "\n",
    "    try:\n",
    "        # Extract data from CSV and database\n",
    "        df_people = extract_data(data_name=\"people\", format_data=\"csv\")\n",
    "        df_relationships = extract_data(data_name=\"relationships\", format_data=\"csv\")\n",
    "        df_acquisition = extract_data(data_name=\"acquisition\", format_data=\"db\")\n",
    "        df_funds = extract_data(data_name=\"funds\", format_data=\"db\")\n",
    "        df_funding_rounds = extract_data(data_name=\"funding_rounds\", format_data=\"db\")\n",
    "        df_company = extract_data(data_name=\"company\", format_data=\"db\")\n",
    "        df_investments = extract_data(data_name=\"investments\", format_data=\"db\")\n",
    "        df_ipos = extract_data(data_name=\"ipos\", format_data=\"db\")\n",
    "\n",
    "        # Transform each dataset separately\n",
    "        # df_transactions = transform_data(df_transactions, \"transactions\")\n",
    "\n",
    "        # Load each transformed dataset into the data warehouse\n",
    "        load_data(df_people, table_name=\"people\")\n",
    "        load_data(df_relationships, table_name=\"relationships\")\n",
    "        load_data(df_acquisition, table_name=\"acquisition\")\n",
    "        load_data(df_funds, table_name=\"funds\")\n",
    "        load_data(df_funding_rounds, table_name=\"funding_rounds\")\n",
    "        load_data(df_company, table_name=\"company\")\n",
    "        load_data(df_investments, table_name=\"investments\")\n",
    "        load_data(df_ipos, table_name=\"ipos\")\n",
    "\n",
    "        logging.info(\"===== Finish Investment Data Pipeline =====\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(\"===== Data Pipeline Failed =====\")\n",
    "        logging.error(e)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b3f6d96-c474-48e1-8547-2f9b7ae2feae",
   "metadata": {},
   "outputs": [],
   "source": [
    "        load_data(df_people, table_name=\"people\")\n",
    "        load_data(df_relationships, table_name=\"relationships\")\n",
    "        load_data(df_acquisition, table_name=\"acquisition\")\n",
    "        load_data(df_funds, table_name=\"funds\")\n",
    "        load_data(df_funding_rounds, table_name=\"funding_rounds\")\n",
    "        load_data(df_company, table_name=\"company\")\n",
    "        load_data(df_investments, table_name=\"investments\")\n",
    "        load_data(df_ipos, table_name=\"ipos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60516a8e-f4eb-4337-b071-ce9d217ce6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ipos = extract_data(data_name=\"ipos\", format_data=\"db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1eb70418-cd57-4c65-ab2c-2b0ea3566243",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data(df_ipos, table_name=\"ipos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51f03d9-7326-42af-b8fc-7bc26fe5a92a",
   "metadata": {},
   "source": [
    "Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "77fe4ba9-219b-4686-9ffc-35095b288a8f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Column' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 37\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreated_at\u001b[39m\u001b[38;5;124m\"\u001b[39m: datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreport\u001b[39m\u001b[38;5;124m\"\u001b[39m: report\n\u001b[1;32m     34\u001b[0m     }\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Run the profiling\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m profile_report \u001b[38;5;241m=\u001b[39m \u001b[43mprofile_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_people\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(profile_report)\n",
      "Cell \u001b[0;32mIn[26], line 8\u001b[0m, in \u001b[0;36mprofile_dataframe\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m      7\u001b[0m     data_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(df[col]\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m----> 8\u001b[0m     missing_percentage \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misnull\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m      9\u001b[0m     entry \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: data_type}\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m missing_percentage \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Column' object is not callable"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def profile_dataframe(df: pd.DataFrame) -> dict:\n",
    "    report = {}\n",
    "    for col in df.columns:\n",
    "        data_type = str(df[col].dtype)\n",
    "        missing_percentage = df[col].isnull().mean() * 100\n",
    "        entry = {\"data_type\": data_type}\n",
    "\n",
    "        if missing_percentage > 0:\n",
    "            entry[\"percentage_missing_value\"] = round(missing_percentage, 2)\n",
    "        else:\n",
    "            entry[\"percentage_missing_value\"] = 0.0\n",
    "\n",
    "        if data_type == 'object' or data_type.startswith('str'):\n",
    "            unique_values = df[col].dropna().unique()\n",
    "            if len(unique_values) <= 10:\n",
    "                entry[\"unique_value\"] = unique_values.tolist()\n",
    "        \n",
    "        if \"date\" in col or \"at\" in col.lower():\n",
    "            try:\n",
    "                valid_dates = pd.to_datetime(df[col], errors='coerce')\n",
    "                percentage_valid_date = valid_dates.notnull().mean() * 100\n",
    "                entry[\"percentage_valid_date\"] = round(percentage_valid_date, 2)\n",
    "            except Exception:\n",
    "                entry[\"percentage_valid_date\"] = 0.0\n",
    "\n",
    "        report[col] = entry\n",
    "\n",
    "    return {\n",
    "        \"created_at\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "        \"report\": report\n",
    "    }\n",
    "\n",
    "# Run the profiling\n",
    "profile_report = profile_dataframe(people_df)\n",
    "print(profile_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "af8649ad-6c15-4976-b35a-378dfd5a53a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, isnan, when, count\n",
    "from utils.helper import init_spark_session\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "def profile_spark(df: pd.DataFrame) -> dict:\n",
    "    spark = init_spark_session()\n",
    "    report = {}\n",
    "    total_rows = df.count()\n",
    "\n",
    "    for col_name in df.columns:\n",
    "        data_type = str(df.schema[col_name].dataType)\n",
    "        \n",
    "        # Hitung missing value (null atau kosong)\n",
    "        missing_count = df.filter(col(col_name).isNull() | (col(col_name) == \"\")).count()\n",
    "        missing_percentage = (missing_count / total_rows) * 100 if total_rows > 0 else 0\n",
    "\n",
    "        entry = {\n",
    "            \"data_type\": data_type,\n",
    "            \"percentage_missing_value\": round(missing_percentage, 2)\n",
    "        }\n",
    "\n",
    "        # Hitung nilai unik (jika tidak terlalu banyak)\n",
    "        unique_values = df.select(col_name).distinct().limit(5).rdd.flatMap(lambda x: x).collect()\n",
    "        entry[\"unique_value\"] = unique_values\n",
    "\n",
    "        # Deteksi valid date\n",
    "        if \"date\" in col_name.lower() or \"at\" in col_name.lower():\n",
    "            try:\n",
    "                valid_dates = df.withColumn(\"parsed\", col(col_name).cast(\"timestamp\"))\n",
    "                valid_count = valid_dates.filter(col(\"parsed\").isNotNull()).count()\n",
    "                entry[\"percentage_valid_date\"] = round((valid_count / total_rows) * 100, 2) if total_rows > 0 else 0\n",
    "            except:\n",
    "                entry[\"percentage_valid_date\"] = 0.0\n",
    "\n",
    "        report[col_name] = entry\n",
    "\n",
    "    profile_result = {\n",
    "        \"person_in_charge\": \"Hudiya Resa\",\n",
    "        \"created_at\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "        \"report\": report\n",
    "    }\n",
    "\n",
    "    # take variable name in df\n",
    "    df_name = [name for name, value in globals().items() if value is df][0]\n",
    "\n",
    "    # Save as JSON\n",
    "    os.makedirs(\"profiling/output\", exist_ok=True)\n",
    "    file_path = f\"profiling/output/{df_name}_profile_{datetime.now().strftime('%Y%m%d')}.json\"\n",
    "    with open(file_path, \"w\") as f:\n",
    "        json.dump(profile_result, f, indent=4, default=str)\n",
    "\n",
    "    print(f\"Profiling report saved to: {file_path}\")\n",
    "    return profile_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "baf897f1-c624-4eaa-afd1-b80bbef418e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling report saved to: profiling/output/people_df_profile_20250423.json\n",
      "{'person_in_charge': 'Hudiya Resa', 'created_at': '2025-04-23', 'report': {'people_id': {'data_type': 'StringType()', 'percentage_missing_value': 0.0, 'unique_value': ['296', '467', '675', '691', '829']}, 'object_id': {'data_type': 'StringType()', 'percentage_missing_value': 0.0, 'unique_value': ['p:73', 'p:171', 'p:214', 'p:486', 'p:625']}, 'first_name': {'data_type': 'StringType()', 'percentage_missing_value': 0.0, 'unique_value': ['Hideki', 'Tyler', 'Rony', 'Laurence', 'Lorne']}, 'last_name': {'data_type': 'StringType()', 'percentage_missing_value': 0.0, 'unique_value': ['Nerst', 'Banerjee', 'Cancel', 'Rekhi', 'Silverstein']}, 'birthplace': {'data_type': 'StringType()', 'percentage_missing_value': 87.61, 'unique_value': ['Gainesville, FL', 'Brisbane, Australia', 'Utah', 'Antwerp', 'Manilla']}, 'affiliation_name': {'data_type': 'StringType()', 'percentage_missing_value': 0.01, 'unique_value': ['Blue Nile', 'Pandora Media', 'YouSendIt', 'Diet TV', 'SomethingSimpler'], 'percentage_valid_date': 0.0}}}\n"
     ]
    }
   ],
   "source": [
    "from utils.helper import init_spark_session\n",
    "from staging.extract.extract_data import extract_data\n",
    "\n",
    "spark = init_spark_session()\n",
    "\n",
    "people_df = extract_data(data_name=\"people\", format_data=\"csv\")\n",
    "profile_report = profile_spark(people_df)\n",
    "print(profile_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "206153a8-cf51-4e41-99f6-9f4c41b66f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling report saved to: profiling/output/df_relationships_profile_20250423.json\n",
      "{'person_in_charge': 'Hudiya Resa', 'created_at': '2025-04-23', 'report': {'relationship_id': {'data_type': 'StringType()', 'percentage_missing_value': 0.0, 'unique_value': ['675', '829', '1090', '1159', '1436'], 'percentage_valid_date': 99.41}, 'person_object_id': {'data_type': 'StringType()', 'percentage_missing_value': 0.0, 'unique_value': ['p:73', 'p:171', 'p:214', 'p:486', 'p:683']}, 'relationship_object_id': {'data_type': 'StringType()', 'percentage_missing_value': 0.0, 'unique_value': ['c:933', 'c:2009', 'c:396', 'c:3146', 'c:3173'], 'percentage_valid_date': 0.0}, 'start_at': {'data_type': 'StringType()', 'percentage_missing_value': 53.48, 'unique_value': ['2010-10-01 00:00:00.000', '2010-05-24 00:00:00.000', '2006-09-19 00:00:00.000', '2006-04-15 00:00:00.000', '1969-01-01 00:00:00.000'], 'percentage_valid_date': 46.52}, 'end_at': {'data_type': 'StringType()', 'percentage_missing_value': 85.71, 'unique_value': ['2010-10-01 00:00:00.000', '2010-05-24 00:00:00.000', '2011-08-03 00:00:00.000', '2009-04-29 00:00:00.000', '2011-08-31 00:00:00.000'], 'percentage_valid_date': 14.29}, 'is_past': {'data_type': 'StringType()', 'percentage_missing_value': 0.0, 'unique_value': ['false']}, 'sequence': {'data_type': 'StringType()', 'percentage_missing_value': 0.0, 'unique_value': ['363048', '97717', '284036', '28463', '48590']}, 'title': {'data_type': 'StringType()', 'percentage_missing_value': 4.03, 'unique_value': ['Founder & CEO', 'VP, Platform & Technology Develop', 'Director of Product Marketing', 'COO/ Founder', 'Director of Developer Platform']}, 'created_at': {'data_type': 'StringType()', 'percentage_missing_value': 0.0, 'unique_value': ['2007-05-31 21:28:58.000', '2007-06-09 08:47:31.000', '2007-07-11 13:08:34.000', '2007-07-28 04:12:50.000', '2007-08-04 06:57:33.000'], 'percentage_valid_date': 100.0}, 'updated_at': {'data_type': 'StringType()', 'percentage_missing_value': 0.0, 'unique_value': ['2010-01-27 00:32:09.000', '2008-04-13 18:34:47.000', '2011-09-05 20:20:58.000', '2013-08-29 19:08:03.000', '2012-09-16 09:05:16.000'], 'percentage_valid_date': 100.0}}}\n"
     ]
    }
   ],
   "source": [
    "profile_report = profile_spark(df_relationships)\n",
    "print(profile_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "256d4dde-2521-4224-9752-7dd50ea579ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling report saved to: profiling/output/df_company_profile_20250423.json\n",
      "{'person_in_charge': 'Hudiya Resa', 'created_at': '2025-04-23', 'report': {'office_id': {'data_type': 'IntegerType()', 'percentage_missing_value': 0.0, 'unique_value': [148, 463, 471, 833, 1238]}, 'object_id': {'data_type': 'StringType()', 'percentage_missing_value': 0.0, 'unique_value': ['c:396', 'c:545', 'c:636', 'c:933', 'c:1558']}, 'description': {'data_type': 'StringType()', 'percentage_missing_value': 41.73, 'unique_value': ['Corporate Headquarters', 'Technology & Life Science', 'Bangalore', 'Cynapse Headquarters', 'HQ office']}, 'region': {'data_type': 'StringType()', 'percentage_missing_value': 0.0, 'unique_value': ['Bangalore', 'West Vancouver', 'Hanover', 'Thessaloniki', 'Magdeburg']}, 'address1': {'data_type': 'StringType()', 'percentage_missing_value': 16.18, 'unique_value': ['576 Seymour Street', '1901 Munsey Drive', '651 Beacon Pkwy W. Suite 201', '1 Half Moon Ln', '30 East 23rd Street; 6th Floor']}, 'address2': {'data_type': 'StringType()', 'percentage_missing_value': 61.34, 'unique_value': ['Suite 515', 'Suit 310,', '197 Reid Street', 'Building 200, Suite 175', 'Section- 6, Mirpur']}, 'city': {'data_type': 'StringType()', 'percentage_missing_value': 4.59, 'unique_value': ['Bangalore', 'Worcester', 'West Vancouver', 'Hanover', 'Thessaloniki']}, 'zip_code': {'data_type': 'StringType()', 'percentage_missing_value': 16.13, 'unique_value': ['95134', '94102', '84606', '75007', '08648']}, 'state_code': {'data_type': 'StringType()', 'percentage_missing_value': 42.36, 'unique_value': ['AZ', 'SC', 'LA', 'MN', 'NJ'], 'percentage_valid_date': 0.0}, 'country_code': {'data_type': 'StringType()', 'percentage_missing_value': 0.0, 'unique_value': ['HTI', 'POL', 'LVA', 'BRB', 'JAM']}, 'latitude': {'data_type': 'DecimalType(9,6)', 'percentage_missing_value': 0.0, 'unique_value': [Decimal('37.590339'), Decimal('37.780134'), Decimal('38.952273'), Decimal('37.523044'), Decimal('47.601882')], 'percentage_valid_date': 100.0}, 'longitude': {'data_type': 'DecimalType(9,6)', 'percentage_missing_value': 0.0, 'unique_value': [Decimal('-122.206893'), Decimal('-71.051931'), Decimal('-111.663926'), Decimal('-94.428499'), Decimal('-122.773646')]}, 'created_at': {'data_type': 'TimestampType()', 'percentage_missing_value': 0.0, 'unique_value': [datetime.datetime(2007, 1, 1, 22, 19, 54)], 'percentage_valid_date': 100.0}, 'updated_at': {'data_type': 'TimestampType()', 'percentage_missing_value': 0.0, 'unique_value': [datetime.datetime(2007, 1, 1, 22, 19, 54)], 'percentage_valid_date': 100.0}}}\n"
     ]
    }
   ],
   "source": [
    "profile_report = profile_spark(df_company)\n",
    "print(profile_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
